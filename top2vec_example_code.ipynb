{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install top2vec\n",
    "!pip install umap-learn[plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import umap.plot\n",
    "from umap.umap_ import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "def load_files():\n",
    "    corpus_dict = {\"volume\": [],\n",
    "                   \"title\":[],\n",
    "                   \"text\":[]}\n",
    "\n",
    "    for i in range(1, 13):\n",
    "        path = f\"Volume_{i + 1}\"\n",
    "        files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        for file in files:\n",
    "            with open(path + \"/\" + file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "                num_parts = 1  # You can change this to the desired number of parts\n",
    "                story_parts = split_story_content(content, num_parts)\n",
    "\n",
    "                # Add volume, title, and each part of the text to the corpus_dict\n",
    "                for part_num, part_content in enumerate(story_parts, 1):\n",
    "                    corpus_dict['volume'].append(i)\n",
    "                    corpus_dict['title'].append(f\"{file[:-4]}_part{part_num}\")\n",
    "                    corpus_dict['text'].append(part_content)\n",
    "\n",
    "                # corpus_dict['volume'].append(i)\n",
    "                # corpus_dict['title'].append(file[:-4])\n",
    "                # corpus_dict['text'].append(content)\n",
    "\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "def split_story_content(content, num_parts):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)\n",
    "\n",
    "    sentences_per_part = len(sentences) // num_parts\n",
    "\n",
    "    story_parts = [sentences[i:i + sentences_per_part] for i in range(0, len(sentences), sentences_per_part)]\n",
    "\n",
    "    return [' '.join(part) for part in story_parts]\n",
    "\n",
    "corpus_dict = load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE  TOP2VEC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_model = Top2Vec(corpus_dict[\"text\"], embedding_model=\"doc2vec\", speed='learn', min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = top2vec_model.get_topic_sizes()\n",
    "\n",
    "num_topics = top2vec_model.get_num_topics()\n",
    "\n",
    "topic_sizes, topic_nums = top2vec_model.get_topic_sizes()\n",
    "for topic_size, topic_num in zip(topic_sizes[:num_topics], topic_nums[:num_topics]):\n",
    "    print(f\"Topic Num {topic_num} has {topic_size} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics and their document clusters\n",
    "topic_words, word_scores, topics = top2vec_model.get_topics(num_topics=num_topics) # Specify the number of topics you want\n",
    "\n",
    "for topic_number in range(num_topics): \n",
    "    for words, scores, num in zip(topic_words[topic_number:], word_scores[topic_number:], topics[topic_number:]):\n",
    "        print(f\"Topic {num}\")\n",
    "        for word, score in zip(words, scores):\n",
    "            print(word, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_model.generate_topic_wordcloud(0, background_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_args_model = {\n",
    "\"n_neighbors\": 2,\n",
    "\"n_components\": 2,\n",
    "\"metric\": \"cosine\",\n",
    "'min_dist':0.9,\n",
    "'spread':1,\n",
    "'random_state': 42\n",
    "}\n",
    "umap_model = umap.UMAP(**umap_args_model).fit(top2vec_model.document_vectors)\n",
    "\n",
    "umap.plot.points(umap_model, labels = top2vec_model.doc_top ) #\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
